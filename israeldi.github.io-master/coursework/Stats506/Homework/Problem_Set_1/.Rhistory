knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
help(opts_chunk)
## Question 2: Including Plots
# read chunk (does not run code)
```{r echo=FALSE}
read_chunk('ps1_q2.R')
```
---
title: "Problem Set 1"
author: "Israel Diego"
date: "October 1, 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Question 1
Use a first level header for each question.
### Part A
Use lower level headers for thematic parts.
### Part B
Use numbered lists for specific parts of each question:
i. Response to part one. Maybe reference supporting tables and graphs.
i. Response to part two.
\pagebreak
## Question 2: Including Plots
# read chunk (does not run code)
```{r echo=FALSE}
read_chunk('ps1_q2.R')
```
help(read_chunk)
# read chunk (does not run code)
```{r echo=FALSE}
knitr::read_chunk('ps1_q2.R')
```
# read chunk (does not run code)
```{r echo=FALSE}
knitr::read_chunk('ps1_q2.R')
```
---
title: "Problem Set 1"
author: "Israel Diego"
date: `r format.Date(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## Question 1
Use a first level header for each question.
### Part A
Use lower level headers for thematic parts.
### Part B
Use numbered lists for specific parts of each question:
i. Response to part one. Maybe reference supporting tables and graphs.
i. Response to part two.
\pagebreak
## Question 2: Including Plots
knitr::opts_chunk$set(echo = FALSE)
knitr::read_chunk('ps1_q2.R')
library(knitr)
knitr::read_chunk('ps1_q2.R')
knitr::read_chunk('ps1_q2.R')
```{r echo = FALSE}
dim(iris)
```{r echo = FALSE}
knitr::read_chunk('ps1_q2.R')
knitr::read_chunk('ps1_q2.R')
knitr::read_chunk('ps1_q2.R')
```{r echo = FALSE}
knitr::read_chunk('ps1_q2.R')
## Problem Set 1 Question 2
##
## The nycflights2014 Data used in this script can be found at the link below:
## https://raw.githubusercontent.com/wiki/arunsrinivasan/flights/NYCflights14/flights14.csv'
## The nycflights2013 Data used in this script can be found in the
## nycflights2013 R package
##
## In this problem set we compare proportions of flights per airline leaving
## three New York Airports in 2013 and 2014.
##
## Author: Israel Diego (israeldi@umich.edu)
## Updated: September 30, 2018 - Last modified date
# 80: -------------------------------------------------------------------------
# Libraries: ------------------------------------------------------------------
# install.packages("nycflights13")
library(nycflights13)
library(tidyverse)
library(knitr)
source(ps1_q2.R)
## Problem Set 1 Question 2
##
## The nycflights2014 Data used in this script can be found at the link below:
## https://raw.githubusercontent.com/wiki/arunsrinivasan/flights/NYCflights14/flights14.csv'
## The nycflights2013 Data used in this script can be found in the
## nycflights2013 R package
##
## In this problem set we compare proportions of flights per airline leaving
## three New York Airports in 2013 and 2014.
##
## Author: Israel Diego (israeldi@umich.edu)
## Updated: September 30, 2018 - Last modified date
# 80: -------------------------------------------------------------------------
# Libraries: ------------------------------------------------------------------
# install.packages("nycflights13")
library(nycflights13)
library(tidyverse)
library(knitr)
source('ps1_q2.R')
knitrsource('ps1_q2.R')
knitr::source('ps1_q2.R')
knitr::source('ps1_q2.R')
source('ps1_q2.R')
rm(list = ls())
source('ps1_q2.R')
---
title: "Problem Set 1"
author: "Israel Diego"
date: `r format.Date(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source('ps1_q2.R')
```
q
f
d
d
jkl;\
source('ps1_q2.R')
source('ps1_q2.R')
source('ps1_q2.R')
rmarkdown::render('ps1.Rmd',
output_file = paste0("index.html"))
library(knitr)
library(gdata)
library(dplyr)
library(faraway)
source('ps2_q3.R', local = TRUE, echo = FALSE)
## Problem Set 2 Question 3
##
## Files: demographics_weights2005.dta
##		 oral_health2005.dta
##  	 imported from the address below:
##
## https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/OHX_D.XPT
## https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT
##
## Author: Israel Diego (israeldi@umich.edu)
## Updated: October 3, 2018 - Last modified date
# install.packages("SASxport")
# install.packages("faraway")
# install.packages("fastDummies")
# install.packages("margins")
# install.packages('gdata')
library(tidyverse)
library(SASxport)
library(fastDummies)
library(margins)
# setwd("~/Google Drive/Documents Google Drive/Fall 2018/Stats 506/Homework/Problem Set 2")
# Functions: ------------------------------------------------------------------
decode_ethnicity = function(x){
# Decodes numeric codes for DIVISION to Census Division labels
#
# Args:
#   x: a single numeric code for a DIVISION
#
# Returns: The division label
if (!is.numeric(x))
stop('decode_division expects numeric input indexed from 1!')
switch(x,
'Mexican',
'Other',
'White',
'Black',
'Other')
}
decode_all_ethnicities = function(x){
# Vectorizes decode_division above
#
# Args:
#  x: a vector of integer-valued DIVISION
#
# Returns: A vector of Census Division labes or a "list" if some are unmatched.
sapply(x, decode_ethnicity)
}
# Importing our Data from the web
oralHealthData =
SASxport::read.xport("OHX_D.XPT")
demogWeightsData =
SASxport::read.xport("DEMO_D.XPT")
# QUESTION 3 Part a -----------------------------------------------------------
# The merged data set drops those patients in Demographic Data that were not
# included in the Oral Health Data
nhanes = merge(oralHealthData, demogWeightsData, by = "SEQN")
# QUESTION 3 Part b -----------------------------------------------------------
nhanes = nhanes %>%
select(SEQN, OHX04HTC, RIDAGEMN) %>%
rename(bicuspidType = OHX04HTC, ageMonths = RIDAGEMN) %>%
filter(!(bicuspidType == 9), !(is.na(bicuspidType)), !(is.na(ageMonths))) %>%
mutate(notPrimary = !(bicuspidType == 1))
fit0 = glm(notPrimary ~ ageMonths, data = nhanes,
family = binomial(link = 'logit'))
summary(fit0)
coeff = fit0$coefficients
# Calculating age at which 25%, 50%, and 75% of individuals that lose primary
# tooth
p = 0.25
age25 = round((-log(1/p - 1) - coeff[1]) / coeff[2])
p = 0.5
age50 = round((-log(1/p - 1) - coeff[1]) / coeff[2])
p = 0.75
age75 = round((-log(1/p - 1) - coeff[1]) / coeff[2])
# Building a range of representative ages between 25 and 75 percentiles,
# incremented by 1
age25 = floor(age25 / 12)
age75 = ceiling(age75 / 12)
representAges = list(age25:age75)
BIC_fit0 = BIC(fit0)
# QUESTION 3 Part c -----------------------------------------------------------
nhanes = merge(oralHealthData, demogWeightsData, by = "SEQN")
nhanes = nhanes %>%
select(SEQN, OHX04HTC, RIDAGEMN, RIAGENDR, RIDRETH1, INDFMPIR) %>%
rename(bicuspidType = OHX04HTC,
ageMonths = RIDAGEMN,
isMale = RIAGENDR,
incomePovertyRatio = INDFMPIR,
ethnicity = RIDRETH1) %>%
filter(!(bicuspidType == 9),
!(is.na(bicuspidType)),
!(is.na(ageMonths))) %>%
mutate(notPrimary = (bicuspidType == 1)) %>%
mutate(ethnicity = decode_all_ethnicities(ethnicity)) %>%
mutate(isMale = sub(2, 0, isMale),
notPrimary = 1 * notPrimary)
nhanes = fastDummies::dummy_cols(nhanes, select_columns = "ethnicity")
# Largest group will be the Reference group
orderEthnicities = nhanes %>%
select(ethnicity) %>%
group_by(ethnicity) %>%
tally() %>%
arrange(desc(n))
# -------------------------------------- #
#  Running Regressions Checking for BIC  #
# -------------------------------------- #
# BIC of original model
BIC(fit0)
# Running our logit regression with Gender
fit1 = glm(notPrimary ~ ageMonths + isMale,
data = nhanes,
family = binomial(link = 'logit'))
# Checking BIC of our model and keep new variable if BIC improves
BIC(fit1) # Gender does not improve BIC
# Running our logit regression with Mexican ethnicity
fit1 = glm(notPrimary ~ ageMonths + ethnicity_Mexican,
data = nhanes,
family = binomial(link = 'logit'))
# Checking BIC of our model and keep new variable if BIC improves
BIC(fit1) # Mexican does not improve BIC
# Running our logit regression with Black ethnicity
fit1 = glm(notPrimary ~ ageMonths + ethnicity_Black,
data = nhanes,
family = binomial(link = 'logit'))
# Checking BIC of our model and keep new variable if BIC improves
BIC(fit1) # Black improves BIC
# Running our logit regression with Other ethnicity
fit1 = glm(notPrimary ~ ageMonths + ethnicity_Black + ethnicity_Other,
data = nhanes,
family = binomial(link = 'logit'))
# Checking BIC of our model and keep new variable if BIC improves
BIC(fit1) # Other does not improve BIC
# Running our logit regression with Income Poverty Ratio
fit1 = glm(notPrimary ~ ageMonths + ethnicity_Black + incomePovertyRatio,
data = nhanes,
family = binomial(link = 'logit'))
# Checking BIC of our model and keep new variable if BIC improves
BIC(fit1) # Income Poverty Ratio improves BIC
# RESULT
# We added Black ethnicity and Income Poverty Ratio to the regression which
# improved BIC
# QUESTION 3 Part d -----------------------------------------------------------
# 1) Computing Adjusted Predictions at the mean for representative ages
nhanes_atmean =
tibble(ageMonths = c(96, 108, 120, 132, 144)) %>%
mutate(ethnicity_Black = mean(nhanes$ethnicity_Black),
incomePovertyRatio = mean(nhanes$incomePovertyRatio, na.rm = TRUE))
m0 = predict(fit0, nhanes_atmean, type = 'response', se = TRUE)
nhanes_atmean = nhanes_atmean %>%
mutate(fit = m0$fit, se = m0$se.fit, lwr = fit - 2*se, upr = fit + 2*se) %>%
mutate(ageMonths = factor(ageMonths, ageMonths[order(fit)]))
# 2) Marginal Effects at the mean using Black categorical variable and Income
# Poverty Ratio at representative ages
meanEthnicityBlack = mean(nhanes$ethnicity_Black)
p_ethnic = c(meanEthnicityBlack, 1 - meanEthnicityBlack)
oral_glm = nhanes %>% select(bicuspidType, ageMonths, ethnicity_Black, incomePovertyRatio)
meanIncomePov = mean(oral_glm$incomePovertyRatio, na.rm = TRUE)
lp_fit0 = c(sum(coef(fit1)*c(1, 96, p_ethnic[1],
meanIncomePov)),
sum(coef(fit1)*c(1, 108, p_ethnic[1],
meanIncomePov)),
sum(coef(fit1)*c(1, 120, p_ethnic[1],
meanIncomePov)),
sum(coef(fit1)*c(1, 132, p_ethnic[1],
meanIncomePov)),
sum(coef(fit1)*c(1, 144, p_ethnic[1],
meanIncomePov))
)
apm = exp(lp_fit0) / {1 + exp(lp_fit0)}
diff(rev(apm))
# 3) Computing Average Marginal Effects using Black categorical variable and
# Income Poverty Ratio at representative ages
new_data =
tibble(intercept = rep(1, each = 10),
ageMonths = rep(c(96,108,120,132,144), 2),
ethnicity_Black = rep(p_ethnic[1], 10),
incomePovertyRatio =
rep(mean(oral_glm$incomePovertyRatio, na.rm = TRUE), 10)
)
new_data = new_data %>%
mutate(fitted = predict(fit1, new_data, type = 'response'))
new_data
aap = new_data %>%
group_by(ageMonths) %>%
summarize(ame = sum(ethnicity_Black*fitted))
c(aap$ame[1] - aap$ame[2],
aap$ame[2] - aap$ame[3],
aap$ame[3] - aap$ame[4],
aap$ame[4] - aap$ame[5])
# Functions: ------------------------------------------------------------------
decode_ethnicity = function(x){
# Decodes numeric codes for DIVISION to Census Division labels
#
# Args:
#   x: a single numeric code for a DIVISION
#
# Returns: The division label
if (!is.numeric(x))
stop('decode_division expects numeric input indexed from 1!')
switch(x,
'Mexican',
'Other',
'White',
'Black',
'Other')
}
decode_all_ethnicities = function(x){
# Vectorizes decode_division above
#
# Args:
#  x: a vector of integer-valued DIVISION
#
# Returns: A vector of Census Division labes or a "list" if some are unmatched.
sapply(x, decode_ethnicity)
}
# Importing our Data from the web
oralHealthData =
SASxport::read.xport("OHX_D.XPT")
demogWeightsData =
SASxport::read.xport("DEMO_D.XPT")
## Problem Set 2 Question 3
##
## Files: demographics_weights2005.dta
##		 oral_health2005.dta
##  	 imported from the address below:
##
## https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/OHX_D.XPT
## https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT
##
## Author: Israel Diego (israeldi@umich.edu)
## Updated: October 3, 2018 - Last modified date
# install.packages("SASxport")
# install.packages("faraway")
# install.packages("fastDummies")
# install.packages("margins")
# install.packages('gdata')
library(tidyverse)
library(SASxport)
library(fastDummies)
library(margins)
# setwd("~/Google Drive/Documents Google Drive/Fall 2018/Stats 506/Homework/Problem Set 2")
# Functions: ------------------------------------------------------------------
decode_ethnicity = function(x){
# Decodes numeric codes for DIVISION to Census Division labels
#
# Args:
#   x: a single numeric code for a DIVISION
#
# Returns: The division label
if (!is.numeric(x))
stop('decode_division expects numeric input indexed from 1!')
switch(x,
'Mexican',
'Other',
'White',
'Black',
'Other')
}
decode_all_ethnicities = function(x){
# Vectorizes decode_division above
#
# Args:
#  x: a vector of integer-valued DIVISION
#
# Returns: A vector of Census Division labes or a "list" if some are unmatched.
sapply(x, decode_ethnicity)
}
# Importing our Data from the web
oralHealthData =
SASxport::read.xport("OHX_D.XPT")
demogWeightsData =
SASxport::read.xport("DEMO_D.XPT")
oralHealthData =
SASxport::read.xport("./OHX_D.XPT")
demogWeightsData =
SASxport::read.xport("./DEMO_D.XPT")
pwd()
pwd
rmarkdown::render('ps1.Rmd',
output_file = paste0("index.html"))
